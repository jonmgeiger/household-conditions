---
title: "Initial Regression Model with Race and HH Conditions"
author: "Jon Geiger, Noel Goodwin, Abigail Joppa"
date: "`r Sys.Date()`"
output: openintro::lab_report
---

```{r load-packages, message=FALSE}
library(tidyverse)
library(tidylog)
library(openintro)
library(leaps)
library(ggthemes)
theme_set(theme_clean())
```

# Purpose

The goal of this analysis is to fit a simple linear regression model to predict average high school district graduation rate based on household conditions and racial composition of the surrounding area. 

# Data Import and Cleaning

```{r}
data <- read_csv("../data/grad_raceP_household.csv") %>%
    select(-student_total)
glimpse(data)
```

We can notice immediately that our original dataset variables such as `pct_pov` and `pct_SP`, are proportions ranging from 0 to 1. We can also notice that the columns representing our racial data such as `pct_hisp_latino` and `pct_white`, are percentages ranging from 0 to 100. In order to run a regression model, we will standardize these predictors and make all fo them range from 0 to 100. This is so that we can interpret a value of "1" as "1%" rather than 100%. 

```{r}
data_standardized <- data %>% 
    mutate(
        across(.cols = pct_pov:pct_CLI, 
               .fns = ~ . * 100)
    )
```

Now that the variables are standardized, let's check out what a simple linear regression model with all variables as predictors would look like. 

# Initial HH Conditions Analysis

We will first construct a simple linear regression model using all household conditions as predictors. 

```{r}
full_df <- data_standardized %>% 
    select(pct_pov:pct_PI, grad_rate_midpt)

hh_df <- full_df %>%
    select(pct_pov:pct_CLI, grad_rate_midpt)

race_df <- full_df %>% 
    select(pct_hisp_latino:pct_PI, grad_rate_midpt)

hh_model <- lm(grad_rate_midpt ~ ., data = hh_df)
race_model <- lm(grad_rate_midpt ~ ., data = race_df)
full_model <- lm(grad_rate_midpt ~ ., data = full_df)

summary(hh_model)
summary(race_model)
summary(full_model)
```

We can immediately notice that some of these predictors are extremely significant in terms of being able to predict graduation rate. Factors such as `pct_pov`, `pct_SP`, `pct_hisp_latino`, `pct_white`, and `pct_asian` are all significant with p-values less than 2e-16. 

We can run a quick ANOVA comparison between the model with just household conditions and the model with all predictors, in order to see if the model with race data provides a statistically significant improvement in the model. 

```{r}
anova(hh_model, full_model)
anova(race_model, full_model)
```

We can see that for both the race-only and the hh-conditions-only models, including the other set of data does provide a significant decrease in the model RSS. 


# Best Subset Analysis

In order to choose the best set of predictors, we will perform best subset analysis, utilizing the `regsubsets()` function from the `leaps` library.

```{r}
fit_full <- regsubsets(grad_rate_midpt ~ ., 
                       data = full_df, 
                       nvmax = ncol(full_df) - 1)
reg_summary <- summary(fit_full)
reg_summary
```

In this output, one row represents one model. We fit thirteen models, each of which is the "best model" for that number of predictors. 

We can immediately notice something interesting: The best one-variable model uses `pct_SP`, or percent of single-parent households, as a predictor. The best two-variable model, however, does not use `pct_SP`, but rather `pct_pov` and `pct_native` as predictors. The best three-variable model utilizes all three of these predictors. 

Let's see which of these models yields the best predictors based on RSS, Adjusted $R^2$, AIC (Akaike Information Criterion), BIC, and Mallow's $C_p$. 

```{r}
par(mfrow = c(2, 2))
plot(reg_summary$rss, xlab = "Number of Variables", 
     ylab = "RSS", type = "l")
plot(reg_summary$adjr2, xlab = "Number of Variables", 
     ylab = "Adjusted RSq", type = "l")
points(which.max(reg_summary$adjr2), reg_summary$adjr2[which.max(reg_summary$adjr2)], col = "red", cex = 2, pch = 20)
plot(reg_summary$cp, xlab = "Number of Variables", 
     ylab = "Cp", type = "l")
points(which.min(reg_summary$cp), reg_summary$cp[which.min(reg_summary$cp)], col = "red", cex = 2, pch = 20)
plot(reg_summary$bic, xlab = "Number of Variables", 
     ylab = "BIC", type = "l")
points (which.min(reg_summary$bic), reg_summary$bic[which.min(reg_summary$bic)], col = "red", cex = 2, pch = 20)
```

We can look more into, say, the Bayesian Information Criterion: 

```{r}
plot(fit_full, scale = "bic")
```

In this case, the top row represents the "best" model, and the bottom row represents the "worst" model as measured by the BIC. The best model includes all variables aside from the percentage of households with no computer/internet access (`pct_NCI`) and the percentage of native americans in the school district (`pct_native`). 

To do: 

- Assess whether variables such as `pct_native` should be considered in the model 
- Use Training/Test/CV sets to avoid overfitting
- Check for multicollinearity
- Lasso regression to adjust for multicollinearity